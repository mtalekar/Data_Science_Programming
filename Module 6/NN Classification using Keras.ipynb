{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Neural Network Classification - Using Keras\n",
    "\n",
    "https://keras.io\n",
    "\n",
    "INSTALL pip install tensorflow and check (keras is part of tensorflow 2.0\n",
    "(my version is older pip install tensorflow followed by pip install keras)\n",
    "\n",
    "We will predict the ocean proximity (`ocean_proximity` column) of Californian districts, given a number of features from these districts.\n",
    "\n",
    "**The unit of analysis is a DISTRICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the missing values\n",
    "housing.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Let's also reset the index\n",
    "housing.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the training and test data sets\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # drop labels \n",
    "\n",
    "#Select the label\n",
    "housing_target = housing[[\"ocean_proximity\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "housing_num_std = scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_num_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the label column\n",
    "\n",
    "Tensorflow wants the labels in integer form. So, we need to do Ordinal Encoding, then convert the numbers to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_labels_ord = ordinal_encoder.fit_transform(housing_target)\n",
    "\n",
    "housing_labels_ord[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type is float. It needs to be integer\n",
    "housing_labels_ord.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to integer\n",
    "\n",
    "housing_labels_int = housing_labels_ord.astype(int)\n",
    "\n",
    "housing_labels_int.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(housing_num_std, housing_labels_int, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification using Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_dim=9, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "#final layer: there has to be 5 nodes with softmax (because we have 5 categories)\n",
    "model.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IF BINARY CLASSIFICATION, change the last layer as follows:\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "sgd = keras.optimizers.SGD(lr=0.05)\n",
    "\n",
    "\n",
    "#You need to use \"categorical_crossentropy\" for mutli-class\n",
    "#but since our target is ordinal, we need to use \"sparse_...\"\n",
    "#if it is binary classification, then use binary_crossentropy\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "model.fit(train_x, train_y, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(test_x, test_y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers, Learning rate, Dropout, Initialization & Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "\n",
    "#Set the learning rate:\n",
    "lr=0.001\n",
    "\n",
    "\n",
    "#Available optimizers:\n",
    "adagrad = keras.optimizers.Adagrad(lr=lr, decay=0.0)\n",
    "sgd = keras.optimizers.SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=False)\n",
    "rmsprop = keras.optimizers.RMSprop(lr=lr, rho=0.9, decay=0.0)\n",
    "adam = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "nesterov_adam = keras.optimizers.Nadam(lr=lr, beta_1=0.9, beta_2=0.999, schedule_decay=0.004)\n",
    "\n",
    "#Initializations:\n",
    "xavier = keras.initializers.glorot_normal(seed=None)\n",
    "he = keras.initializers.he_normal(seed=None)\n",
    "\n",
    "\n",
    "# Activation functions. Uncomment only one\n",
    "#activation = 'elu' \n",
    "activation = 'relu'\n",
    "#activation = 'tanh'\n",
    "#activation = 'sigmoid'\n",
    "\n",
    "\n",
    "\n",
    "#See the droput layers below:\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_dim=9, activation=activation, kernel_initializer=xavier))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(25, activation=activation, kernel_initializer=xavier))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation=activation, kernel_initializer=xavier))\n",
    "\n",
    "#final layer: there has to be 5 nodes with softmax (because we have 5 categories)\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "#Compile\"\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=nesterov_adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(train_x, train_y, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(test_x, test_y)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping based on validation results\n",
    "\n",
    "To do this, you need to send the validation data sets to the fit() function and use a callback.\n",
    "\n",
    "EarlyStopping Arguments:\n",
    "\n",
    "**monitor:** quantity to be monitored.<br>\n",
    "**min_delta:** minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.<br>\n",
    "**patience:** number of epochs with no improvement after which training will be stopped.<br>\n",
    "**verbose:** verbosity mode.<br>\n",
    "**mode:** one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity.<br>\n",
    "**baseline:** Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline.<br>\n",
    "**restore_best_weights:** whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "callback = [earlystop]\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(test_x, test_y), \n",
    "          epochs=100, batch_size=100, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_x, test_y)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
